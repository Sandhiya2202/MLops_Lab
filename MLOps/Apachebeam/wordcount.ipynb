{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LAB_ROOT = \".\"   # stay in current folder\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "import os; print(\"Working in:\", os.getcwd()); print(\"Folders ready: data/, outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Once upon a time, a curious student built a small machine that could learn.\n",
    "The machine read thousands of pages, numbers, and songs, trying to make sense of the world.\n",
    "Every day it grew a little smarter and a little kinder.\n",
    "It learned to help people cook, drive, and dream.\n",
    "But one day the student asked, “Will you ever replace me?”\n",
    "The machine smiled in silence and wrote on the screen:\n",
    "  I was never meant to replace you.\n",
    "  I was meant to remind you how powerful you are when you learn.\n",
    "\n",
    "From that day, the student and the machine worked together,\n",
    "creating art, solving problems, and teaching others to imagine a better world.\"\"\"\n",
    "with open(\"data/ai_future_story.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "print(\"Created data/ai_future_story.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mirrors the classic Beam wordcount sample: run(argv=None), --input/--output flags, named transforms.\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class SplitWords(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        line = element.lower()\n",
    "        line = re.sub(r\"[^a-z0-9\\s']\", \" \", line)  # keep letters, digits, space, apostrophes\n",
    "        for w in line.split():\n",
    "            if w:\n",
    "                yield w\n",
    "\n",
    "def format_result(word_count):\n",
    "    word, count = word_count\n",
    "    return f\"{word}\\t{count}\"\n",
    "\n",
    "def run(argv=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--input\",\n",
    "        default=\"data/kinglear.txt\",   # same default as the sample; we’ll override below\n",
    "        help=\"Input path (file or glob), e.g., data/ai_future_story.txt or data/*.txt\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        default=\"outputs/wordcount_output\",\n",
    "        help=\"Output prefix (Beam adds shard suffixes), e.g., outputs/wordcount_ai_future\",\n",
    "    )\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "    os.makedirs(os.path.dirname(known_args.output) or \".\", exist_ok=True)\n",
    "\n",
    "    pipeline_options = PipelineOptions(pipeline_args)  # add runner args here if you need\n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        lines = p | \"Read\" >> beam.io.ReadFromText(known_args.input)\n",
    "        words = lines | \"Split\" >> beam.ParDo(SplitWords())\n",
    "        pairs = words | \"PairWithOne\" >> beam.Map(lambda w: (w, 1))\n",
    "        counts = pairs | \"CountPerWord\" >> beam.CombinePerKey(sum)\n",
    "        _ = (counts\n",
    "             | \"Format\" >> beam.Map(format_result)\n",
    "             | \"Write\" >> beam.io.WriteToText(known_args.output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the file we just created\n",
    "\n",
    "argv = [\n",
    "    \"--input\",  \"data/ai_future_story.txt\",\n",
    "    \"--output\", \"outputs/wordcount_ai_future\",\n",
    "    # You can also pass runner options here, e.g. \"--runner\", \"DirectRunner\"\n",
    "]\n",
    "run(argv)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "parts = sorted(glob.glob(\"outputs/wordcount_ai_future*\"))\n",
    "print(\"Output shards:\", parts)\n",
    "\n",
    "for p in parts:\n",
    "    print(f\"\\n=== {os.path.basename(p)} ===\")\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        print(f.read(), end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, pandas as pd\n",
    "\n",
    "parts = sorted(glob.glob(\"outputs/wordcount_ai_future-*\"))\n",
    "rows = []\n",
    "for p in parts:\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \"\\t\" in line:\n",
    "                w, c = line.strip().split(\"\\t\")\n",
    "                rows.append((w, int(c)))\n",
    "\n",
    "df = (pd.DataFrame(rows, columns=[\"word\",\"count\"])\n",
    "        .groupby(\"word\", as_index=False).sum()\n",
    "        .sort_values(\"count\", ascending=False))\n",
    "\n",
    "df.to_csv(\"outputs/story_counts_full.csv\", index=False)\n",
    "print(\"Wrote outputs/story_counts_full.csv with\", len(df), \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, glob, os\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# make sure the input exists\n",
    "if not os.path.exists(\"data/ai_future_story.txt\"):\n",
    "    open(\"data/ai_future_story.txt\",\"w\",encoding=\"utf-8\").write(\n",
    "        \"hello beam hello world\\nbeam makes word count easy\\n\"\n",
    "    )\n",
    "\n",
    "INPUT_PATH = \"data/ai_future_story.txt\"\n",
    "OUTPUT_PREFIX = \"outputs/wordcount_ai_future\"\n",
    "\n",
    "def normalize_line(s):\n",
    "    return re.sub(r\"[^a-z0-9\\s']\", \" \", s.lower())\n",
    "\n",
    "with beam.Pipeline(options=PipelineOptions([\"--runner=DirectRunner\"])) as p:\n",
    "    (p\n",
    "     | \"Read\"   >> beam.io.ReadFromText(INPUT_PATH)\n",
    "     | \"Norm\"   >> beam.Map(normalize_line)\n",
    "     | \"Split\"  >> beam.FlatMap(lambda l: l.split())\n",
    "     | \"Pair1\"  >> beam.Map(lambda w: (w,1))\n",
    "     | \"Sum\"    >> beam.CombinePerKey(sum)\n",
    "     | \"Format\" >> beam.Map(lambda kv: f\"{kv[0]}\\t{kv[1]}\")\n",
    "     | \"Write\"  >> beam.io.WriteToText(OUTPUT_PREFIX)\n",
    "    )\n",
    "\n",
    "sorted(glob.glob(\"outputs/wordcount_ai_future*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, shutil\n",
    "\n",
    "parts = sorted(glob.glob(\"outputs/wordcount_ai_future*\"))\n",
    "if not parts:\n",
    "    raise SystemExit(\"No shards found under outputs/. Re-run the pipeline cell first.\")\n",
    "\n",
    "out_txt = \"outputs/story_counts_full.txt\"\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as w:\n",
    "    for p in parts:\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            shutil.copyfileobj(f, w)\n",
    "\n",
    "print(\"Merged\", len(parts), \"file(s) ->\", out_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows=[]\n",
    "for p in parts:\n",
    "    for line in open(p, \"r\", encoding=\"utf-8\"):\n",
    "        if \"\\t\" in line:\n",
    "            w,c = line.strip().split(\"\\t\")\n",
    "            rows.append((w,int(c)))\n",
    "df = (pd.DataFrame(rows, columns=[\"word\",\"count\"])\n",
    "        .groupby(\"word\", as_index=False).sum()\n",
    "        .sort_values(\"count\", ascending=False))\n",
    "df.to_csv(\"outputs/story_counts_full.csv\", index=False)\n",
    "print(\"Wrote outputs/story_counts_full.csv with\", len(df), \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
